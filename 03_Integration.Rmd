---
title: "Data integration"
author: "Drake Williams"
date: "`r BiocStyle::doc_date()`"
output: html_document
knit: (function(inputFile, encoding) { 
    rmarkdown::render(
        inputFile, encoding = encoding, 
        output_file = file.path(
            dirname(inputFile), paste0('03_Integration_',Sys.Date(),'.html'))) 
                })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
  warning = F,
  message = F, 
  message = F,
  out.width = "100%", 
                      fig.align='center',
  time_it = TRUE)
options(width = 1200)
```

# Introduction

The objective of this notebook is to prepare data for analysis (downsampling, integration, etc)

## Load packages

```{r load packages}
library(pacman)
pacman::p_load(Seurat, ggplot2, scCustomize)
```

## Load datasets
```{r load datasets}
combined <- readRDS(file = "/home/williamsdrw/epithelial-IL23/04_data_objects/01_SoupX_objects/2023-06-02_015-allWithMeta.RDS")
```

## Normalize seurat objects
Initially I tried to use SCT integration, however this gave me the following error during the integration step: Error in .T2C(newTMat(i = c(ij1[, 1], ij2[, 1]), j = c(ij1[, 2], ij2[, : unable to coerce from TsparseMatrix to [CR]sparseMatrixwhen length of 'i' slot exceeds 2^31-1. Several others have had this error, but seurat devs have not been very helpful in resolving.

So, following the tutorial (https://satijalab.org/seurat/articles/integration_large_datasets.html)[here], use the reference approach for integrating large datasets.
```{r normalization}
# run normalization on datasets divided by sample
# combined_bySample <- SplitObject(combined, split.by = "orig.ident")
# combined_byTissue <- SplitObject(combined, split.by = "tissue")
combined_byRef <- SplitObject(combined, split.by = "ref")

combined_byRef <- lapply(X = combined_byRef, FUN = function(x) {
    x <- NormalizeData(x, verbose = FALSE)
    x <- FindVariableFeatures(x, verbose = FALSE)
    })

features <- SelectIntegrationFeatures(object.list = combined_byRef)
combined_byRef <- lapply(X = combined_byRef, FUN = function(x) {
    x <- ScaleData(x, features = features, verbose = FALSE)
    x <- RunPCA(x, features = features, verbose = FALSE)
})

# this is the workflow for doing SCT integration, which does not work well for this very large dataset
# combined_bySample <- lapply(combined_bySample,
#                             RunPCA,
#                             npcs = 30, 
#                             verbose = F)
#                             
# # need to set maxSize for PrepSCTIntegration to work
# options(future.globals.maxSize = 2000 * 1024^2) # set allowed size to 2K MiB
# 
# 
# feats <- SelectIntegrationFeatures(combined_bySample,
#   nfeatures = 3000,
#   verbose = T
# )
# combined_bySample <- PrepSCTIntegration(
#   object.list = combined_bySample,
#   anchor.features = feats,
#   verbose = T
# )

```
## Integrate seurat objects
If using SCT, this step takes several hours if the dataset is moderately large. Not much will happen on screen but it is working in the background.
This step takes too long to complete in the RMD (times out) so need to do it locally.

If using RPCA + reference, it takes considerably less (less than 1hr), so you can run it as an RMD
```{r integration}
# Find integration anchors
# Note: these two steps take a long time.

anchors <- FindIntegrationAnchors(object.list = combined_byRef, reference = c(1,4,8), reduction = "rpca",
    dims = 1:50)
combined.int <- IntegrateData(anchorset = anchors, dims = 1:50)

```

## Save new objects
```{r save objects}
baseDir <- "~/epithelial-IL23/04_data_objects/03_Analysis_objects/"
saveRDS(combined.int, file = paste0(baseDir, Sys.Date(), "_03-IntegrationOutput.RDS"))
```

## Session Info
```{r session info}
sessionInfo()
```


